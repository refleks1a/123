#------ Modified Secant

# def modified_secant(x0, delta=1e-5):
#     iterations = 0
#     while iterations < max_iterations:
#         x1 = x0 - f(x0) * delta / (f(x0 + delta) - f(x0))
#         if abs(f(x1)) <= tolerance:
#             return x1, iterations
#         x0 = x1
#         iterations += 1
#     return x1, iterations

#------ Newton-Raphson for system

# def f(xyz):
#     x = xyz[0]
#     y = xyz[1]
#     z = xyz[2]
#     return np.array([2*x+y-5+2*z**2, y**3+4*z-4, x*y+z-e**z])

# def df(xyz):
#     x = xyz[0]
#     y = xyz[1]
#     z = xyz[2]
#     return np.array([[2, 1, 4*z], [0, 3*y**2, 4], [y, x, 1-e**z]])

# def newton(f, df, xyz, tolerance = 1e-7, max_iterations = 50):
    
#     for i in range(max_iterations):
#         if np.linalg.norm(f(xyz)) <= tolerance:
#             return xyz
#         xyz = xyz - np.linalg.inv(df(xyz)).dot(f(xyz))
#     return None

# xyz = [1, 1, 0]

# res = newton(f, df, xyz)
# print(res)
# print(f(res))

#------- Gauss Elimination with scaled partial pivoting 

# def gauss_pivoting(A, b):
#     n=len(b)
#     l=list(range(n))  #index vector
#     s = [max(x) for x in A] #scale vector
           
#     for i in range(n - 1):
#         max_ratio = 0.0
#         for j in range(i, n):
#             ratio = abs(A[l[j]][i]) / s[l[j]]
#             if ratio > max_ratio:
#                 max_ratio = ratio
#                 pivot=j
                
        
#         l[i], l[pivot]=l[pivot], l[i]
        
#         for j in range(i + 1, n):
#             factor = A[l[j]][i] / A[l[i]][i]
#             for k in range(i, n):
#                 A[l[j]][k] -=  factor * A[l[i]][k]
#             b[l[j]] -= factor * b[l[i]]

#     x=[0]*n

#     x[n - 1] = b[l[n - 1]] / A[l[n - 1]][n - 1]
#     for j in range(n - 2, -1, -1):
#         summ = 0.0
#         for k in range(j + 1, n):
#             summ += A[l[j]][k] * x[k]
#         x[j] = (b[l[j]] - summ) / A[l[j]][j]
#     return x


# A = [[2, 5, 4, 1], [1, 3, 2, 1], [2, 10, 9, 7], [3, 8, 9, 2]]
# b = [20, 11, 40, 37]

# roots=np.linalg.inv(A).dot(b)

#------- Curve fitting
# x=array([1,2,3,4,5,6])
# y=array([0.5, 0.6, 0.8, 1.0, 1.2, 1.7])

# A=array([[len(x), sum(x)],
#         [sum(x), sum(x**2)]])
# B=array([sum(y), sum(y*x)])

# print(A,B)
# PAR=linalg.solve(A,B)

# print(f"Python solve {PAR}")
# plt.scatter(x,y, label="data")

# param, xcov=curve_fit(model, x,y)

# print(param)

# ym=model(x, *param)

# print(ym)
# plt.plot(x, ym, label="least squares")
# plt.grid()
# plt.legend()
# xn=4.5
# yn=model(xn, *param)
# plt.plot(xn, yn, "*r")
# plt.show()
# print(f"f(x)={param[0]:6.2g}*x+{param[1]:6.2g}", sep="")
# resid=abs(y-ym)
# print("REsiduals:", resid)
# FI=sum(resid**2)
# print(f"Error function: {FI}")

#------- Bisection/Newton-Raphson/Secant

# def bisection(a,b,error):
# 	k = 0
# 	while abs(a-b)/2 > error:
# 		k+=1
# 		c = (a+b)/2
# 		if f(a) * f(c) < 0:
# 			b = c
# 		else:
# 			a = c
# 	return c, k


# def newton_raphson(x, f, df, n, error):
#     for i in range(n):
#         x = x - f(x)/df(x)
#         if abs(f(x)) <= error:
#             return x, i
#         if df(x) == 0:
#             print("Derivative is zero.!")
#             return None, None
#     return None, None

# def secant(x0, x1, f, n, error):
#     for i in range(n):
#         if f(x1) == f(x0):
#             print("Division by zero")
#             return None, None
#         x2 = x0 - f(x0)*(x1-x0)/(f(x1)-f(x0))
#         if abs(f(x2)) <= error:
#             return x2, i 
#         x1, x0 = x2, x1
#     return None, None


#------- Newton Interpolation

# def divided_diff(x,y):
#     n=len(x)
#     coef=np.zeros([n,n])
#     coef[:, 0]=y
#     for j in range (1, n):
#         for i in range (n-j):
#             coef[i][j]=(coef[i+1][j-1]-coef[i][j-1])/(x[i+j]-x[i])
#     return coef

# def Newton_poly(coef, x_data, x):
#     n=len(x_data)
#     f=0
#     for i in range (n):
#         p=1
#         for j in range (i):
#             p*=(x-x_data[j])
#         f+=coef[i]*p
#     return f

# x=np.array([1.6, 2, 2.5, 3.2, 4, 4.5])
# y=np.array([2, 8, 14, 15, 8, 2])

# coef=divided_diff(x,y)[0, :]
# print(f"coefficients of Newton polynomial: {coef}")

# x_predict=[2.1, 3, 4.2, 6]   # f(6)-?
# x_new=np.linspace(1, 7, 100)

# y_new=Newton_poly(coef, x, x_new)
# y_predict=Newton_poly(coef, x, x_predict)


#------- Lagrange Interpolation

# def Lagrange_poly(x_data, y_data, x):
#     n=len(x_data)
#     f=0
#     for i in range (n):
#         l=1
#         for j in range (n):
#             if i==j: continue
#             l*=(x-x_data[j])/(x_data[i]-x_data[j])
#         f+=y_data[i]*l
#     return f

# x=np.array([1,3,5,7,13])
# y=np.array([800, 2310, 3090, 3940, 4755])
# plt.plot(x, y, "ro", label="Given data")
# x_new=np.linspace(0, 15, 100)
# y_new=Lagrange_poly(x,y, x_new)
# plt.plot(x_new, y_new, label="Lagrange polynomial")
# x_add=10
# y_add=Lagrange_poly(x,y,x_add)
# print(f"VElocity of parashutist for {x_add} sec is {y_add} sm/sec")
# plt.plot(x_add, y_add, "gp", label="Velocity for 10 sec")


#------- Curve fitting with least squares
# import numpy as np
# from numpy import array, linalg
# import matplotlib.pyplot as plt
# from scipy.optimize import curve_fit

# def model(x, a, b, c):  
#     return a*x**2+b*x+c

# x=np.linspace(1, 7, 13)
# print(x)
# y=array([7,6,6,7,8,10,13,15,19,25,33,43,54])
# A=array([[sum(x**4), sum(x**3), sum(x**2)],
#         [sum(x**3), sum(x**2), sum(x)],
#         [sum(x**2), sum(x), len(x)]])
# B=array([sum(y*x**2), sum(y*x), sum(y)])
# print(A)
# print(B)
# PAR=linalg.solve(A,B)
# print(f"Python solve {PAR}")
# plt.scatter(x,y, label="data")
# param, xcov=curve_fit(model, x,y)
# print(param)
# ym=model(x, *param)
# print(ym)
# plt.plot(x, ym, label="least squares")
# plt.grid()
# plt.legend()
# xn=4.5
# yn=model(xn, *param)
# plt.plot(xn, yn, "*r")
# plt.show()
# print(f"f(x)={param[0]:6.2g}*x+{param[1]:6.2g}", sep="")
# resid=abs(y-ym)
# print("REsiduals:", resid)
# FI=sum(resid**2)
# print(f"Error function: {FI}")

